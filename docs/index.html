<!DOCTYPE html>
<html lang="en">
  <head>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="stylesheet" href="style.css" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ViewFormer: NeRF-free Neural Rendering from Few Images Using Transformers</title>
    <meta name="description" content="This webpage presents the ViewFormer paper and additional results. ViewFormer is a NeRF-free neural rendering model based on the transformer architecture. The model is capable of both novel view synthesis and camera pose estimation. It is evaluated on previously unseen 3D scenes.">
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet">
    <!-- Microsoft Clarity analytics -->
    <script type="text/javascript">(function(c,l,a,r,i,t,y){c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);})(window, document, "clarity", "script", "hnv3xao2e7");</script>

  </head>
  <body>
    <header>
      <h1>
        <span class="title-main">ViewFormer</span>
        <span class="title-small">NeRF-free Neural Rendering from Few Images Using Transformers</span>
      </h1>
      <div class="conference">ECCV 2022</div>
    </header>
    <div class="authors">
      <div class="author">
        <span class="author-name">
          <a href="https://jkulhanek.github.io/">Jonáš Kulhánek</a>
        </span>
        <span class="author-affiliation">Czech Technical University in Prague</span>
      </div>
      <div class="author">
        <span class="author-name">
          <a href="http://people.ciirc.cvut.cz/~derneeri/">Erik Derner</a>
        </span>
        <span class="author-affiliation">Czech Technical University in Prague</span>
      </div>
      <div class="author">
        <span class="author-name">
          <a href="https://tsattler.github.io/">Torsten Sattler</a>
        </span>
        <span class="author-affiliation">Czech Technical University in Prague</span>
      </div>
      <div class="author">
        <span class="author-name">
          <a href="http://www.robertbabuska.com/">Robert Babuška</a>
        </span>
        <span class="author-affiliation">Delft University of Technology</span>
      </div>
    </div>
    <div class="links">
      <div class="link link-paper">
        <a href="https://arxiv.org/pdf/2203.10157.pdf">Paper</a>
      </div>
      <div class="link link-github">
        <a href="https://github.com/jkulhanek/viewformer/">Code</a>
      </div>
      <div class="link link-video">
        <a href="https://youtu.be/Jc301FPhZXg">Video</a>
      </div>
      <!--<div class="link link-demo">
        <a href="https://colab.research.google.com/github/jkulhanek/viewformer/blob/master/notebooks/viewformer-playground.ipynb">Demo</a>
      </div>-->
    </div>
    <div class="video">
      <video width="320" height="240" loop autoplay controls muted>
-        <source src="https://data.ciirc.cvut.cz/public/projects/2022ViewFormer/videos/intro-video-notitle.mp4" type="video/mp4">
-      </video>
    </div>

    <section class="abstract">
      <h2>Abstract</h2>
      <p>
Novel view synthesis is a long-standing problem.
In this work, we consider a variant of the problem where we are given only a few context views sparsely covering a scene or an object. 
The goal is to predict novel viewpoints in the scene, which requires learning priors.
The current state of the art is based on Neural Radiance Fields (NeRFs). 
While achieving impressive results, NeRF-based approaches suffer from long training times as they require evaluating thousands of 3D point samples via a deep neural network for each image. 
We propose a simple 2D-only method
that maps context views and a query pose to a new image in a single pass of a neural network.
Our model uses a two-stage architecture consisting of a codebook and a transformer model. The codebook is used to embed individual images into a smaller latent space, and the transformer solves the view synthesis task in this more compact space.
      <img src="resources/overview.svg" alt="ViewFormer architecture overview" style="width: 100%; margin: 1em auto 0.3em auto; display: block;" />
To train our model efficiently, we introduce a novel <i>branching attention</i> mechanism that allows us to use the same model not only for neural rendering but also for camera pose estimation.
Experimental results on real-world scenes show that our approach is competitive compared to NeRF-based methods while not reasoning in 3D, and it is faster to train.
    </section>
    <section>
      <h2>Video</h2>
      <div class="video d16x9">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/Jc301FPhZXg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </div>
    </section>
    <section class="results-co3d">
      <h2>Common Objects in 3D dataset results</h2>
      <p>In this section we present the qualitative results for the Common Objects in 3D (CO3D) dataset [<a class="citation" href="#ref-reizenstein2021common">1</a>]. In the video, we show the model trained on all ten classes used in [<a class="citation" href="#ref-reizenstein2021common">1</a>]. Notice how the quality of the generated image improves with increasing context size.
      </p>
      <div class="video">
        <video width="320" height="240" loop autoplay muted>
          <source src="https://data.ciirc.cvut.cz/public/projects/2022ViewFormer/videos/co3d.mp4" type="video/mp4">
        </video>
      </div>
    </section>
    <section class="results-shapenet">
      <h2>ShapeNet dataset results</h2>
      <p>We also present the results on the ShapeNet dataset.
        ViewFormer is compared with PixelNeRF method [<a class="citation" href="#ref-yu2021pixelnerf">4</a>].
      </p>
      <div class="video">
        <video width="320" height="240" loop autoplay muted>
          <source src="https://data.ciirc.cvut.cz/public/projects/2022ViewFormer/videos/shapenet-cars-chairs-notitle.mp4" type="video/mp4">
        </video>
      </div>
    </section>
    <section class="results-co3d">
      <h2>InteriorNet dataset results</h2>
      <p>In this section we present the qualitative results for the InteriorNet dataset [<a class="citation" href="#ref-li2018interiornet">2</a>]. Notice that some details in the image are lost because the model is not able to represent the scene perfectly.
      </p>
      <div class="video">
        <video width="320" height="240" loop autoplay muted>
          <source src="https://data.ciirc.cvut.cz/public/projects/2022ViewFormer/videos/interiornet.mp4" type="video/mp4">
        </video>
      </div>
    </section>
    <section class="results-7scenes">
      <h2>7-Scenes dataset results</h2>
      <p>The 7-Scenes dataset [<a class="citation" href="#ref-glocker2013real">3</a>] was used primarily to evaluate the performance of camera pose estimation (results given in the paper). However, we also show qualitative results for the novel view synthesis task. In this case, the model was overfitting the training data, which resulted in lower quality of the generated images. We show images from all seven scenes.
      </p>
      <div class="video">
        <video width="320" height="240" loop autoplay muted>
          <source src="https://data.ciirc.cvut.cz/public/projects/2022ViewFormer/videos/7scenes.mp4" type="video/mp4">
        </video>
      </div>
    </section>
    <section class="references">
      <h2>References</h2>
      <div id="ref-reizenstein2021common">
        [1] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. <i>Common objects in 3D: Large-scale learning and evaluation of real-life 3D category reconstruction.</i> In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10901–10911, 2021.
      </div>
      <div id="ref-li2018interiornet">
        [2] Wenbin Li, Sajad Saeedi, John McCormac, Ronald Clark, Dimos Tzoumanikas, Qing Ye, Yuzhong Huang, Rui Tang, and Stefan Leutenegger. <i>InteriorNet: Mega-scale multi sensor photo-realistic indoor scenes dataset.</i> In British Machine Vision Conference (BMVC), 2018.
      </div>
      <div id="ref-glocker2013real">
        [3] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. <i>Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images.</i> In CVPR, 2013.
      </div>
      <div id="yu2021pixelnerf">
        [4] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. <i>PixelNeRF: Neural Radiance Fields from One or Few Images.</i> In CVPR, 2021.
      </div>
    </section>
    <section class="citation">
      <h2>Citation</h2>
      <span>Please use the following citation:</span>
      <pre>
@inproceedings{kulhanek2022viewformer,
  title={ViewFormer: NeRF-free Neural Rendering from Few Images Using Transformers},
  author={Kulh{\'a}nek, Jon{\'a}{\v{s}} and Derner, Erik and Sattler, Torsten and Babu{\v{s}}ka, Robert},
  booktitle={European Conference on Computer Vision (ECCV)},
  year={2022},
}</pre>
    </section>
<a href="https://github.com/jkulhanek/viewformer/" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
  </body>
</html>
